# Evaluation Criteria: Success Metrics & Test Plan

**Task:** T02.4.1_JD-DATA024_DefineClassificationTaxonomy  
**Evidence Artifact:** Evaluation Criteria & Validation Strategy  
**Created By:** DATA-024 (Ontology and Taxonomy Designer)  
**Date:** 2026-01-14

---

## Executive Summary

32-category taxonomy validated against 6 success metrics. Classification accuracy target: 90%+ across all categories. Routing precision: 99%+ (no tag assignment errors). Test plan includes synthetic documents, edge cases, and user acceptance testing.

---

## Success Metrics (SLAs)

### Metric 1: Classification Accuracy

**Definition:** Percentage of documents correctly classified (confidence >= 0.85)

**Target:** ≥ 90% overall accuracy (measured across all 32 categories)

**Measurement:**
```
Accuracy = (# correctly classified) / (# total classified) × 100

Category-specific targets:
  Financial: 90% (transaction data is usually clear)
  Legal: 92% (complex language; higher accuracy needed)
  HR: 91% (sensitive; higher accuracy needed)
  Operations: 88% (lower priority; acceptable threshold lower)
  Other: 75% (low confidence okay; requires manual review anyway)
```

**Test Scenarios:**
- 100 real financial documents (invoices, receipts, statements)
- 50 real legal documents (contracts, notices, compliance docs)
- 50 real HR documents (employment contracts, reviews)
- 40 real operations documents (policies, meeting minutes)
- 10 ambiguous edge cases (documents that could fit multiple categories)

**Acceptance Criteria:**
- Financial accuracy ≥ 90%
- Legal accuracy ≥ 92%
- HR accuracy ≥ 91%
- Operations accuracy ≥ 88%
- Edge cases resolved correctly (manual review if confidence < 0.70)

**Measurement Timeline:** Baseline at T02.4.2 completion; tracked continuously during operations.

### Metric 2: Routing Precision

**Definition:** Percentage of tags correctly assigned to classified documents (no false positives, no missing tags)

**Target:** ≥ 99% precision (mistakes are very rare)

**Measurement:**
```
Precision = (# correctly tagged documents) / (# total tagged documents) × 100

Tag Coverage:
  For each category, verify all expected tags present
  For each document, verify only expected tags present
```

**Test Scenarios:**
- 500 classified documents (from M1 test set)
- Verify tag count matches expected per category
- Verify tag names match lookup table exactly
- Verify no duplicate tags assigned

**Acceptance Criteria:**
- ≥ 99% of documents have correct tag set
- ≤ 1% of documents have missing or extra tags

**Example Validation:**
```
Document: Invoice (classified @ 0.92)
Expected tags: Financial, TransactionDoc, PaymentRequest, SensitiveData
Actual tags: Financial, TransactionDoc, PaymentRequest, SensitiveData ✓
Missing: None ✓
Extra: None ✓
Precision check: PASS
```

### Metric 3: Confidence Distribution

**Definition:** Classification confidence scores should follow expected distribution (high confidence for clear documents, lower for ambiguous)

**Target:** Median confidence ≥ 0.88; no more than 5% of classifications < 0.75

**Measurement:**
```
Distribution = percentile breakdown of confidence scores

Expected distribution (ideal):
  Percentile   Confidence   Interpretation
  10%          0.75         Some low-confidence edge cases
  50%          0.88         Half are highly confident
  90%          0.95         Most documents are clear
  95%+         0.98         Very confident clear cases
```

**Test Scenarios:**
- Histogram of confidence scores across 500 test documents
- Identify outliers (very low confidence = potential LLM error)
- Identify missing categories (if distribution shows no documents classified as a category, investigate)

**Acceptance Criteria:**
- Median confidence ≥ 0.88
- ≤ 5% of classifications < 0.75 (ambiguous cases requiring review)
- ≥ 40% of classifications ≥ 0.95 (clear cases)

### Metric 4: Category Coverage

**Definition:** All 32 categories are used; no unused categories

**Target:** 100% of categories present in test set (all 32 categories classified at least once)

**Measurement:**
```
Coverage = (# categories with ≥ 1 document) / 32

Category distribution should reflect business distribution:
  Financial: 40-50% of documents (business-heavy)
  Legal: 15-20% of documents
  HR: 15-20% of documents
  Operations: 15-20% of documents
  Other: 5-10% of documents (low volume expected)
```

**Test Scenarios:**
- 250-document diverse sample across all categories
- Ensure at least 2-3 documents per category
- Verify rare categories (e.g., "Termination") are still correctly classified

**Acceptance Criteria:**
- All 32 categories have ≥ 1 document in test set
- Financial domain: 100-150 documents
- Legal domain: 30-50 documents
- HR domain: 30-50 documents
- Operations domain: 30-50 documents
- Other domain: 10-20 documents

### Metric 5: Edge Case Handling

**Definition:** Ambiguous documents are handled correctly (escalated for review if confidence < 0.70)

**Target:** 100% of low-confidence documents tagged "RequiresReview"

**Measurement:**
```
EdgeCaseHandling = (# low-confidence docs with RequiresReview tag) / (# low-confidence docs)

Edge cases identified:
  1. Multi-category documents (e.g., receipt + warranty)
  2. Foreign language documents
  3. Corrupted/scanned documents (faint text)
  4. Ambiguous terminology (invoice vs. receipt)
  5. Non-standard format (unusual layout)
```

**Test Scenarios:**
- 20 edge case documents (deliberately ambiguous)
- Verify confidence scores are low (< 0.70)
- Verify "RequiresReview" tag is applied
- Verify audit log captures reason for escalation

**Acceptance Criteria:**
- 100% of documents with confidence < 0.70 have "RequiresReview" tag
- No low-confidence documents classified without review escalation
- Audit log captures reason for each escalation

### Metric 6: Compliance Categorization Correctness

**Definition:** High-sensitive documents are correctly identified and tagged with compliance level

**Target:** 100% of high-sensitivity documents (Legal, HR, Tax) tagged correctly

**Measurement:**
```
ComplianceCategorization = (# high-sensitivity docs with correct compliance tag) / (# high-sensitivity docs)

High-sensitivity categories:
  TaxDocument → HighSensitivity + High compliance
  Termination → HighSensitivity + High compliance
  IntellectualProperty → HighSensitivity + High compliance
  LegalNotice → HighSensitivity + High compliance
  Payroll → HighSensitivity + High compliance
  Compliance → HighSensitivity + High compliance
```

**Test Scenarios:**
- 50 high-sensitivity documents from each sensitive category
- Verify tags include "HighSensitivity"
- Verify compliance_level = "High"
- Verify encryption/access restrictions are configured

**Acceptance Criteria:**
- 100% of high-sensitivity documents tagged correctly
- 0 high-sensitivity documents incorrectly tagged as low-sensitivity
- All high-sensitivity documents flagged for encryption

---

## Test Plan

### Phase 1: Taxonomy Validation (Syntax Check)

**Duration:** 2 days (automated)

**Tests:**
1. Parse all 32 categories (valid JSON/YAML)
2. Verify no duplicate category names
3. Verify confidence thresholds are 0.00-1.00
4. Verify examples/non-examples are present for all categories
5. Verify no circular references in ontology

**Output:** Taxonomy is syntactically valid; ready for LLM training.

### Phase 2: Classification Training (LLM-Specific)

**Duration:** 3 days (LLM training)

**Tests:**
1. Fine-tune LLM on 100 sample documents (10 per category)
2. Evaluate training loss (convergence on examples)
3. Run inference on holdout set (30 documents, unseen)
4. Calculate initial accuracy (baseline for comparison)

**Success Criteria:**
- Training loss converges (no divergence)
- Baseline accuracy ≥ 75% on unseen documents

**Output:** Trained classifier ready for Phase 3 validation.

### Phase 3: Classification Accuracy Testing (Metric 1)

**Duration:** 5 days (manual + automated review)

**Test Set Composition:**
- 100 financial documents (10 invoices, 10 receipts, 10 POs, 10 quotes, 10 statements, 10 credit memos, 10 debit memos, 10 expense reports, 5 tax, 5 payments)
- 50 legal documents (8 contracts, 8 SOWs, 6 liability, 6 T&C, 6 compliance, 6 IP, 5 legal notices, 5 regulatory)
- 50 HR documents (12 contracts, 12 payroll, 12 perf reviews, 12 benefits, 2 training)
- 40 operations documents (10 plans, 10 minutes, 8 org charts, 8 policies, 4 vendor)
- 10 edge cases (ambiguous documents)

**Process:**
1. Run classifier on all 250 documents
2. For each classification, capture: category, confidence, timestamp
3. Cross-reference with ground truth (human validation)
4. Calculate accuracy by category and overall
5. Document false positives and false negatives

**Acceptance Criteria:**
- Overall accuracy ≥ 90%
- Financial ≥ 90%, Legal ≥ 92%, HR ≥ 91%, Ops ≥ 88%
- False positive categories identified and tuned

**Output:** Classification accuracy baseline established; LLM tuning complete.

### Phase 4: Routing & Tagging Testing (Metric 2)

**Duration:** 3 days (tag assignment verification)

**Test Set:** 500 documents from Phase 3 + new samples

**Process:**
1. Run classification → routing pipeline on 500 documents
2. For each document, verify:
   - Category matches expected from ground truth
   - All expected tags present
   - No extra/spurious tags present
   - Confidence matches classification output
3. Generate tag assignment audit report

**Acceptance Criteria:**
- ≥ 99% of documents have correct tag set
- ≤ 1% of documents have missing or extra tags
- No duplicate tags per document

**Output:** Routing pipeline validated; tag assignment accuracy proven.

### Phase 5: Edge Case & Exception Testing (Metric 5)

**Duration:** 3 days (manual + automated)

**Edge Cases to Test:**
1. **Multi-purpose documents** (receipt + warranty):
   - Classification: Should pick primary (receipt)
   - Confidence: 0.65-0.75 (ambiguous)
   - Resolution: RequiresReview tag + manual queue
   
2. **Foreign language** (Spanish invoice):
   - Classification: Should still detect "Invoice" by structure
   - Confidence: 0.70-0.80 (language barrier)
   - Resolution: RequiresReview tag (optional re-classification)
   
3. **Corrupted/scanned** (faint OCR):
   - Classification: Should fail to classify
   - Confidence: 0.40-0.60 (unreadable)
   - Resolution: RequiresReview tag + manual queue
   
4. **Ambiguous terminology** ("Invoice" used incorrectly):
   - Classification: May pick wrong category
   - Confidence: 0.60-0.70 (conflicting signals)
   - Resolution: RequiresReview tag + manual review
   
5. **Non-standard format** (table-based invoice):
   - Classification: May pick wrong category if not trained on format
   - Confidence: 0.65-0.75 (unusual structure)
   - Resolution: RequiresReview tag + retraining if needed

**Process:**
1. Create 20 edge case documents (deliberately problematic)
2. Run classifier on all 20
3. Verify confidence scores are < 0.85 for ambiguous cases
4. Verify RequiresReview tag is applied
5. Verify audit log captures reason

**Acceptance Criteria:**
- 100% of edge cases < 0.85 confidence
- 100% of edge cases tagged RequiresReview
- No edge case classified with high confidence (> 0.90)

**Output:** Edge case handling validated; escalation process proven.

### Phase 6: Compliance Accuracy Testing (Metric 6)

**Duration:** 2 days (compliance-focused)

**Test Set:** 50 documents per high-sensitivity category (300 total)

**Sensitivity Categories to Test:**
- 50 tax documents (TaxDocument → HighSensitivity)
- 50 terminations (Termination → HighSensitivity)
- 50 IP documents (IntellectualProperty → HighSensitivity)
- 50 legal notices (LegalNotice → HighSensitivity)
- 50 payroll (Payroll → HighSensitivity)

**Process:**
1. Run classifier on 250 high-sensitivity documents
2. Verify compliance_level = "High" for all
3. Verify HighSensitivity tag present
4. Verify encryption/access restrictions configured

**Acceptance Criteria:**
- 100% of high-sensitivity documents have correct compliance level
- 0 high-sensitivity documents misclassified as low-sensitivity
- All high-sensitivity documents flagged for encryption

**Output:** Compliance handling validated; security controls confirmed.

### Phase 7: Integration Testing (D02.5 Readiness)

**Duration:** 5 days (full pipeline)

**Test Scenarios:**
1. **End-to-end classification:**
   - Document imported (D02.1) → classified (T02.4.2) → tagged (D02.5) → stored (D02.3)
   - Verify all steps execute in < 5 seconds per document
   
2. **Faceted search queries:**
   - "Show all Financial documents" (faceted by domain)
   - "Show all HighSensitivity documents" (faceted by compliance)
   - "Show all documents with confidence < 0.70" (faceted by confidence)
   - Verify queries return < 500ms for 1M documents
   
3. **Audit trail completeness:**
   - Import → Classification → Tag assignment all logged
   - Verify audit log captures category, confidence, timestamp, user
   - Verify no audit gaps

**Acceptance Criteria:**
- All end-to-end tests < 5 seconds per document
- Faceted queries < 500ms for 1M documents
- 100% audit trail completeness

**Output:** Full pipeline integration validated; ready for production deployment.

### Phase 8: User Acceptance Testing (UAT)

**Duration:** 1 week (stakeholder validation)

**Participants:**
- Finance team (evaluate financial categories)
- Legal team (evaluate legal categories)
- HR team (evaluate HR categories)
- Operations team (evaluate operations categories)

**Test Scenarios:**
1. **Finance UAT:**
   - Classify 50 real financial documents
   - Verify Invoice, Receipt, PO, Statement are correct
   - Confirm examples/non-examples match business reality
   - Document feedback for v1.1 improvements
   
2. **Legal UAT:**
   - Classify 25 real legal documents
   - Verify Contract, SOW, LegalNotice categories match business usage
   - Confirm compliance routing is correct
   - Document feedback
   
3. **HR UAT:**
   - Classify 25 real HR documents
   - Verify Employment Contract, Payroll, Termination align with HR processes
   - Confirm sensitive data handling is appropriate
   - Document feedback
   
4. **Operations UAT:**
   - Classify 20 operations documents
   - Verify Policy, ProjectPlan, MeetingMinutes are usable
   - Confirm routing/tagging supports operations workflows
   - Document feedback

**Acceptance Criteria:**
- ≥ 80% user satisfaction (survey)
- No critical issues (0 blocking defects)
- Feedback logged for v1.1 improvements

**Output:** Taxonomy approved by all stakeholders; ready for production.

---

## Test Metrics Summary Table

| Metric | Target | Test Phase | Pass/Fail |
|--------|--------|-----------|-----------|
| M1: Classification Accuracy | ≥ 90% overall | Phase 3 | ? |
| M1: Financial Accuracy | ≥ 90% | Phase 3 | ? |
| M1: Legal Accuracy | ≥ 92% | Phase 3 | ? |
| M1: HR Accuracy | ≥ 91% | Phase 3 | ? |
| M1: Operations Accuracy | ≥ 88% | Phase 3 | ? |
| M2: Routing Precision | ≥ 99% | Phase 4 | ? |
| M3: Confidence Distribution | Median ≥ 0.88 | Phase 3 | ? |
| M4: Category Coverage | 100% (all 32) | Phase 3 | ? |
| M5: Edge Case Handling | 100% escalated | Phase 5 | ? |
| M6: Compliance Accuracy | 100% high-sensitivity | Phase 6 | ? |
| Integration: Latency | < 5s per doc | Phase 7 | ? |
| Integration: Query Performance | < 500ms | Phase 7 | ? |
| UAT: Stakeholder Satisfaction | ≥ 80% | Phase 8 | ? |

---

## Defect Resolution Process

**Critical (Blocks Release):**
- Classification accuracy < 88% in any domain
- Routing precision < 97%
- Edge case handling failures > 5%
- High-sensitivity document misclassification (ANY)

**Action:** Pause release; investigate root cause; retrain LLM or adjust taxonomy.

**Major (Non-Blocking):**
- Classification accuracy 88-90% (acceptable range)
- Routing precision 97-99% (acceptable range)
- UAT feedback on category definitions

**Action:** Log for v1.1 improvements; proceed with release.

**Minor:**
- LLM prompt wording suggestions
- Example clarifications

**Action:** Include in v1.1 update.

---

**Document Status:** Evaluation criteria and test plan complete; ready for implementation readiness documentation.
