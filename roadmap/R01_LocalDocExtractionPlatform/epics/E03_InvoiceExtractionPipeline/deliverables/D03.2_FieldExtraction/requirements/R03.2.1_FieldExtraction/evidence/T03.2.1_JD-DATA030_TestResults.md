# Test Results â€” T03.2.1_JD-DATA030_LLMExtractionPipeline

**Date:** 2026-01-18

## Scope
This evidence validates that invoice extraction can use local Ollama inference (HTTP, localhost-only) and return schema-valid `InvoiceExtraction` results with `engine=llm`.

## Environment
- Ollama reachable at `http://localhost:11434`
- Model present: `llama3.1:8b`
- API server: FastAPI via Uvicorn on `http://127.0.0.1:8000`

## Automated Tests
Command:
- `python -m pytest -q`

Result:
- `50 passed`

## Live Smoke Test (End-to-End)
Configuration used:
- `DOCEXTRACTOR_OLLAMA_ENABLED=1`
- `DOCEXTRACTOR_OLLAMA_HOST=http://localhost:11434`
- `DOCEXTRACTOR_OLLAMA_MODEL=llama3.1:8b`

Invoice upload:
- Endpoint: `POST /api/invoices/extract` (multipart file upload)

Observed response (key fields):
- `extraction_metadata.engine = llm`
- `extraction_metadata.model = llama3.1:8b`
- `vendor.value = ACME Supplies`
- `invoice_number.value = INV-1001`
- `total.value = $17.50`
- `line_items` validates and returns 2 items

Ollama runtime evidence:
- `ollama ps` shows `llama3.1:8b` loaded on GPU immediately after the request.

## Notes
- The extraction pipeline now coerces common LLM output variants (notably `line_items` subfields) to match the strict `InvoiceExtraction` schema to avoid unnecessary fallback to rules when the LLM succeeded.
