# T02.4.2 ‚Äì Design Classifier Prompts (PARALLEL WORKSTREAM)

**Task Owner:** AGENT-002 (Prompt Systems Engineer)  
**Assigned:** 2026-01-14T23:15Z  
**Expected Completion:** 2026-01-15T05:15Z (6h duration)  
**Priority:** üü° PARALLEL (independent of critical path)

---

## Task Context & Requirements

### Input Specifications
- **From T02.4.1 (DATA-024):** Classification taxonomy with:
  - 32 document categories
  - Ontology structure
  - Routing logic
  - Example documents per category
  - Evaluation framework

### Deliverables Expected
1. **Classification Prompts (3 variants)**
   - **System Prompt:** Classification system role, task description, output format
   - **Few-Shot Prompt:** Examples for each of 32 categories (representative samples)
   - **Evaluation Prompt:** Structured output schema (JSON with category, confidence, reasoning)

2. **Prompt Specifications Document**
   - Each prompt's purpose and usage
   - Input/output contract (format, constraints)
   - Expected accuracy targets (from T02.4.1 evaluation plan)
   - Edge cases and fallback behavior
   - Version control metadata

3. **Test Harness**
   - Test dataset (50‚Äì100 documents across all 32 categories)
   - Expected classification for each test document
   - Evaluation metrics (accuracy, per-category precision/recall)
   - Baseline accuracy target: >90%

4. **Integration Specification**
   - How classifier will be invoked by pipeline
   - Error handling (unclassifiable documents, low confidence)
   - Prompt versioning strategy
   - A/B testing setup for future improvements

### Success Criteria
- ‚úÖ 3 prompt variants designed (system, few-shot, eval)
- ‚úÖ Clear input/output contracts
- ‚úÖ Test harness created with >90% accuracy baseline
- ‚úÖ Prompts handle all 32 categories consistently
- ‚úÖ Versioned and documented for DATA-029 (T02.4.3)

---

## Execution Context for AGENT-002

**Your Expertise Area:** Prompt design, tool-use specifications, structured outputs, evaluation  
**This Task Requires:** Classification prompts with measurable accuracy targets

### Key Considerations
1. **Prompt Quality is Critical:** Target >90% accuracy on test set
   - Few-shot examples must be representative (at least 1 per category)
   - System prompt should be explicit about task constraints
   - Output schema must be unambiguous (JSON structure)

2. **Evaluation Compatibility:** Your prompts feed T02.4.3 (DATA-029)
   - Include clear accuracy/confidence metrics in output
   - Design for confusion matrix analysis
   - Enable per-category performance tracking

3. **Integration Points:**
   - Prompts will run via Ollama (local LLM)
   - Input: document text or metadata
   - Output: JSON with category + confidence + reasoning
   - Latency target: <2s per document (for pipeline throughput)

4. **Edge Cases Matter:**
   - Multi-category documents (ambiguous classifications)
   - Out-of-distribution documents (new categories)
   - Very short/very long documents
   - Non-English or mixed-language content

### Recommended Approach
- Start with **clear taxonomy analysis** (T02.4.1 output)
- Design **system prompt** that establishes classification discipline
- Create **few-shot examples** (best examples for each category)
- Specify **JSON output contract** (category, confidence, reasoning)
- Build **test dataset** from T02.4.1 samples
- Validate with **baseline accuracy measurement**

### Example Output Structure
```json
{
  "document_id": "doc_123",
  "primary_category": "Invoice",
  "confidence": 0.95,
  "secondary_categories": ["Purchase Order"],
  "reasoning": "Document contains itemized charges, total amount, and vendor information typical of invoices.",
  "model_version": "T02.4.2_v1",
  "prompt_version": "system_v1"
}
```

---

## Blockers & Dependencies

**Depends On:** T02.4.1 (Define Classification Spec) ‚Äì ‚úÖ COMPLETE  
**Blocks:** T02.4.3 (Test Classification Accuracy, DATA-029) ‚Äì awaiting prompts  
**No Critical Path Impact:** Parallel workstream (can continue while T02.2.2-3 execute)

---

## Output Location

**Evidence Artifact:** `/Reserved/DocExtractor/evidence/R02.4.2_ClassificationPrompts/`  
**Filename Convention:** `T02.4.2_JD-AGENT002_PromptDesign.md`

---

## Success Metrics

| Metric | Target | Validation |
|--------|--------|-----------|
| Accuracy on Test Set | >90% | Evaluated by DATA-029 |
| Categories Covered | 32/32 | Prompt design review |
| Few-Shot Examples | ‚â•1 per category | Prompt review |
| Output Format Consistency | 100% valid JSON | Schema validation |
| Latency per Document | <2s | Ollama benchmark |

---

## Escalation SLA

- ‚è±Ô∏è **If blocked >2 hours:** Notify PM-007
- ‚è±Ô∏è **If blocking DATA-029 (T02.4.3):** High priority escalation
- üì¢ **Communication:** Async task updates

---

## Next Steps After Completion

1. DATA-029 begins T02.4.3 (Test Classification Accuracy)
   - Runs your prompts on test dataset
   - Measures accuracy, per-category metrics
   - Provides feedback (iterate if needed)

2. If accuracy <90%:
   - Iterate on prompts (data-driven refinement)
   - Add more/better few-shot examples
   - Refine system prompt for consistency

3. If accuracy ‚â•90%:
   - Prompts signed off by QC-101
   - Ready for Phase 2 integration into pipeline

**Parallel execution means you can proceed independently. Accuracy is your gate.**
