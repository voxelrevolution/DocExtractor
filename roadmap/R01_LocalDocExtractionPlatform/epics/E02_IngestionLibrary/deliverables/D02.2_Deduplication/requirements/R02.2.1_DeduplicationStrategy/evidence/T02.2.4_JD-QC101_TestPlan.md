# T02.2.4: Test Plan â€“ Deduplication Correctness Validation

**Task:** T02.2.4_JD-QC101_TestDeduplicationCorrectness  
**QA Lead:** QC-101 (External Validator)  
**Test Date:** 2026-01-14T16:10Z  
**Implementation Under Test:** T02.2.3 (DEV-034) Deduplication Logic  
**Expected Duration:** 6 hours  

---

## Test Scope

This test plan validates the deduplication system across:
1. **Hash Correctness** â€“ Deterministic SHA-256 via pgcrypto
2. **Duplicate Detection** â€“ 100% accuracy, zero false negatives
3. **False Positive Prevention** â€“ No legitimate different files marked as duplicates
4. **Performance** â€“ Hash <50ms, Lookup <10ms
5. **Audit Logging** â€“ Completeness and immutability
6. **Collision Handling** â€“ Graceful handling (if applicable)

---

## Test Environment Setup

**Database:** PostgreSQL 15.2 (test instance)  
**Schema:** hash_index + duplicate_log tables (from T02.2.2 DDL)  
**Test Data:**
- 100 legitimate documents (various sizes: 10KBâ€“50MB)
- 50 exact duplicates (byte-for-byte identical to originals)
- 30 near-duplicates (1-byte modifications)

**Performance Baseline:**
- Hash computation SLO: <50ms per document (implementation target: <0.8ms)
- Lookup SLO: <10ms per query (implementation target: <1ms)

---

## Test Cases

### Test 1: Hash Correctness â€“ Exact File Reproduction

**Objective:** Verify SHA-256 produces identical hash for same file content

**Test Steps:**
1. Load document A (e.g., invoice_001.pdf, 2.5MB)
2. Compute hash #1 via `compute_document_hash('invoice_001.pdf')`
3. Reload document A from disk (fresh read)
4. Compute hash #2 via `compute_document_hash('invoice_001.pdf')`
5. Compare: hash #1 = hash #2?

**Expected Result:** âœ… PASS  
Hash #1 and hash #2 are identical (both produce same SHA-256)

**Pass Criteria:** hash #1 === hash #2

---

### Test 2: Hash Differentiation â€“ Different Files

**Objective:** Verify different files produce different hashes

**Test Steps:**
1. Load document A (invoice_001.pdf)
2. Load document B (invoice_002.pdf, different content)
3. Compute hash_A = `compute_document_hash('invoice_001.pdf')`
4. Compute hash_B = `compute_document_hash('invoice_002.pdf')`
5. Compare: hash_A â‰  hash_B?

**Expected Result:** âœ… PASS  
Different files produce different hashes (collision-resistant)

**Pass Criteria:** hash_A !== hash_B

---

### Test 3: Duplicate Detection â€“ Exact Duplicate

**Objective:** Verify system detects and rejects exact duplicates

**Test Steps:**
1. Insert document A into import batch
2. Compute hash_A, run `detect_and_record_duplicate(import_id=1, doc_id=1, hash=hash_A)`
   - Result: is_duplicate = FALSE (first upload, new document)
3. Verify hash_A inserted into hash_index
4. Insert duplicate of document A into second import batch
5. Compute hash_A again, run `detect_and_record_duplicate(import_id=2, doc_id=2, hash=hash_A)`
   - Result: is_duplicate = TRUE, original_document_id = 1

**Expected Result:** âœ… PASS  
First upload accepted; second upload rejected as duplicate (matches doc #1)

**Pass Criteria:**
- First call returns is_duplicate=FALSE
- Second call returns is_duplicate=TRUE with original_document_id=1

---

### Test 4: No False Positives â€“ Similar But Different Files

**Objective:** Verify system doesn't reject legitimate different files

**Test Steps:**
1. Document A (invoice_001.pdf, contains "Invoice #123")
2. Document B (invoice_001_v2.pdf, contains "Invoice #124" â€“ ONE byte different)
3. Compute hash_A, run detect_and_record_duplicate (import_id=1)
   - Result: is_duplicate = FALSE
4. Compute hash_B (different hash due to 1-byte change)
5. Run detect_and_record_duplicate with hash_B (import_id=1)
   - Result: is_duplicate = FALSE?

**Expected Result:** âœ… PASS  
Both documents accepted as unique (different hashes, not duplicates)

**Pass Criteria:** is_duplicate = FALSE for both (no false positive)

---

### Test 5: Performance â€“ Hash Computation (100 Documents)

**Objective:** Verify hash computation stays under 50ms SLO

**Test Steps:**
1. Generate 100 test documents (sizes: 100KBâ€“10MB)
2. Measure time to compute hash for each document
3. Record: min, max, average, 95th percentile
4. Compare to SLO: <50ms per document

**Expected Result:** âœ… PASS  
Average hash time <50ms; 95th percentile <100ms

**Pass Criteria:** Average < 50ms; Max < 200ms

---

### Test 6: Performance â€“ Database Lookup (10k Queries)

**Objective:** Verify duplicate lookup stays under 10ms SLO

**Test Steps:**
1. Pre-populate hash_index with 100,000 documents (various hashes)
2. Run 10,000 lookups using random hashes from the set
3. Measure query time for each lookup (using EXPLAIN ANALYZE)
4. Record: min, max, average, 95th percentile

**Expected Result:** âœ… PASS  
Average lookup <10ms; 95th percentile <20ms

**Pass Criteria:** Average < 10ms; 95th percentile < 20ms

---

### Test 7: Audit Trail Completeness

**Objective:** Verify all duplicate decisions are logged with full context

**Test Steps:**
1. Run import batch with 50 documents (40 new, 10 duplicates)
2. Query duplicate_log table
3. Verify row count = 10 (exactly one log entry per detected duplicate)
4. Spot-check log entries for completeness:
   - import_id (present)
   - duplicate_file_name (present)
   - duplicate_sha256 (present, matches computed hash)
   - original_document_id (present, matches original)
   - decision (set to 'reject')
   - decided_at (timestamp present)

**Expected Result:** âœ… PASS  
duplicate_log contains 10 entries; all fields populated correctly

**Pass Criteria:**
- Row count = 10
- All required fields present and non-null
- Hashes match expected values

---

### Test 8: Audit Trail Immutability

**Objective:** Verify audit log entries cannot be modified post-creation

**Test Steps:**
1. Create duplicate_log entry (via detect_and_record_duplicate)
2. Attempt to UPDATE the created_at timestamp (should fail)
3. Attempt to UPDATE the decision field (should fail)
4. Verify no UPDATE triggers execute

**Expected Result:** âœ… PASS  
All UPDATE attempts fail (no audit log mutations allowed)

**Pass Criteria:** UPDATE statements fail with permission error or constraint violation

---

### Test 9: Collision Handling (If Applicable)

**Objective:** Verify system handles SHA-256 collisions gracefully (or note if impossible)

**Test Steps:**
1. Attempt to create artificial hash collision (e.g., insert same hash for two different document_ids)
2. Observe system behavior:
   - Option A: Collision impossible (SHA-256 collision-resistant, skip this test)
   - Option B: System logs collision, documents marked as duplicates
   - Option C: System rejects second document, logs error

**Expected Result:** âœ… PASS or SKIP  
Collision handling is graceful (either impossible or logged/rejected cleanly)

**Pass Criteria:** No system crash; collision either prevented or safely logged

---

## Test Execution Order

1. **Hash Correctness** (Tests 1â€“2) â€“ Foundation
2. **Duplicate Detection** (Tests 3â€“4) â€“ Core functionality
3. **Performance** (Tests 5â€“6) â€“ SLO validation
4. **Audit Trail** (Tests 7â€“8) â€“ Compliance
5. **Edge Cases** (Test 9) â€“ Robustness

---

## Success Criteria (All Required)

- âœ… Test 1: Hash Correctness â€“ PASS
- âœ… Test 2: Hash Differentiation â€“ PASS
- âœ… Test 3: Duplicate Detection â€“ PASS (zero false negatives)
- âœ… Test 4: No False Positives â€“ PASS
- âœ… Test 5: Hash Performance â€“ PASS (<50ms)
- âœ… Test 6: Lookup Performance â€“ PASS (<10ms)
- âœ… Test 7: Audit Trail Complete â€“ PASS
- âœ… Test 8: Audit Trail Immutable â€“ PASS
- âœ… Test 9: Collision Handling â€“ PASS or SKIP

**Overall Success:** All tests PASS (or reasonably SKIP)

---

## Known Limitations & Assumptions

1. **SHA-256 Collision Likelihood:** Astronomically low (2^-256 probability per pair); not testing exhaustively
2. **File System Caching:** OS may cache repeated reads; hash times may vary; using EXPLAIN ANALYZE for DB times only
3. **Concurrency:** Single-threaded test; concurrent deduplication tested separately in T02.2.5
4. **File Sizes:** Test range 100KBâ€“10MB; extremely large files (>1GB) tested separately if required

---

## Next Steps (Upon Completion)

- [ ] Document all test results in T02.2.4_JD-QC101_TestResults.md
- [ ] Log any issues in T02.2.4_JD-QC101_IssuesAndResolution.md
- [ ] Performance analysis in T02.2.4_JD-QC101_PerformanceValidation.md
- [ ] Final sign-off in T02.2.4_JD-QC101_FinalSignOff.md
- [ ] Update PROJECT_STATUS_DASHBOARD with completion timestamp

---

**Test Plan Created:** 2026-01-14T16:10Z  
**QA Lead:** QC-101  
**Status:** ðŸŸ¢ Ready to Execute
