# T02.3.2 â€“ Integration Notes & Interface Specifications

**Task:** T02.3.2_JD-DEV034_CreateMigrations  
**Owner:** DEV-034 (Database Reliability Engineer)  
**Date:** 2026-01-15T10:45Z  
**Status:** âœ… COMPLETE  

---

## Migration Integration Points

### 1. Integration with T02.3.1 (SQL Schema Design)

**Mapping:** Migration V001 implements T02.3.1 schema exactly

| T02.3.1 Design | V001 Implementation | Status |
|----------------|-------------------|--------|
| documents table | CREATE TABLE documents (13 cols) | âœ… Match |
| hash_index table | CREATE TABLE hash_index (7 cols) | âœ… Match |
| audit_log table | CREATE TABLE audit_log (8 cols) | âœ… Match |
| error_log table | CREATE TABLE error_log (6 cols) | âœ… Match |
| Primary keys | SERIAL + PrimaryKeyConstraint | âœ… Match |
| Foreign keys | ON DELETE CASCADE enforced | âœ… Match |
| Unique constraints | file_hash UNIQUE on both tables | âœ… Match |
| Indices (8) | All 8 indices created | âœ… Match |
| Defaults | NOW() for timestamps | âœ… Match |

---

### 2. Integration with D02.2 (Deduplication)

**Interface:** hash_index table

**How DEV-024 (or DEV-027) will use it:**

```python
# Dedup implementation (T02.2.3) will:

# 1. Check if hash exists
SELECT document_id FROM hash_index WHERE file_hash = %s

# 2. If not found, insert document and hash_index entry
INSERT INTO documents (...) RETURNING id
INSERT INTO hash_index (file_hash, document_id, first_seen, duplicate_count, last_seen)
  VALUES (%s, %s, NOW(), 0, NOW())

# 3. If found, increment duplicate counter
UPDATE hash_index SET duplicate_count = duplicate_count + 1, last_seen = NOW()
  WHERE file_hash = %s

# 4. Insert audit log
INSERT INTO audit_log (document_id, action, action_timestamp, user_or_system, ...)
  VALUES (%s, 'ingested', NOW(), 'batch_import', ...)
```

**Performance:** O(1) lookup via index on file_hash

---

### 3. Integration with D02.1 (Document Importer)

**Interface:** documents table

**How batch import (T02.1.3) will use it:**

```python
# Batch import implementation will:

# 1. For each document in batch:
INSERT INTO documents (
  file_name, file_size_bytes, file_hash, file_type,
  ingestion_source, ingestion_timestamp, document_text, metadata_extracted,
  ingestion_status, error_message, created_at, updated_at
) VALUES (...) RETURNING id

# 2. On success: ingestion_status = 'success', error_message = NULL
# 3. On failure: ingestion_status = 'failed', error_message = 'reason'
# 4. Insert error_log if needed:
INSERT INTO error_log (file_name, error_type, error_message, error_timestamp, ...)
  VALUES (%s, 'extraction_error', %s, NOW(), ...)
```

**Key Fields:**
- `ingestion_source`: batch_import (for batch), api (for API), manual (for UI)
- `ingestion_status`: success, failed, retry
- `error_message`: NULL on success, populated on failure

---

### 4. Integration with D02.4 (Classification)

**Interface:** audit_log table + documents table

**How classification (T02.4.2-T02.4.3) will use it:**

```python
# Classification implementation will:

# 1. Query documents with classification status = NULL or 'pending'
SELECT id, document_text FROM documents
WHERE classification_status IS NULL
ORDER BY ingestion_timestamp DESC LIMIT 100

# 2. After classification, update audit log:
INSERT INTO audit_log (
  document_id, action, action_timestamp, user_or_system,
  old_value, new_value, change_reason
) VALUES (
  %s, 'classified', NOW(), 'classification_engine',
  JSON('{"category": null}'),
  JSON('{"category": "invoice"}'),
  'Auto-classified by model'
)

# 3. Update document metadata:
UPDATE documents
SET metadata_extracted = jsonb_set(metadata_extracted, '{classification}', %s)
WHERE id = %s
```

**Key Fields:**
- `audit_log.action`: 'classified' (for tracking)
- `audit_log.user_or_system`: 'classification_engine' or specific model name
- `documents.metadata_extracted`: JSON with classification results

---

### 5. Integration with D02.5 (Tagging & Organization)

**Interface:** audit_log table + extensible metadata

**How tagging (T02.5.1-T02.5.2) will use it:**

```python
# Tagging implementation will:

# 1. Query documents by classification category:
SELECT id, metadata_extracted FROM documents
WHERE metadata_extracted->>'classification' = 'invoice'
ORDER BY ingestion_timestamp DESC

# 2. After tagging, log action:
INSERT INTO audit_log (
  document_id, action, action_timestamp, user_or_system,
  old_value, new_value
) VALUES (
  %s, 'tagged', NOW(), 'tagging_system',
  JSON('{"tags": []}'),
  JSON('{"tags": ["finance", "2025-q1"]}')
)

# 3. Update metadata:
UPDATE documents
SET metadata_extracted = jsonb_set(metadata_extracted, '{tags}', %s)
WHERE id = %s
```

---

## Database Connection Configuration

### Environment Variables (Recommended)

```bash
# Application config
export DATABASE_URL="postgresql://user:password@localhost/documents"
export DATABASE_POOL_SIZE=10
export DATABASE_MAX_OVERFLOW=20
```

### Application Connection Code (Python Example)

```python
import os
from sqlalchemy import create_engine

# Read from environment
database_url = os.environ.get('DATABASE_URL', 'postgresql://localhost/documents')

# Create connection pool
engine = create_engine(
    database_url,
    pool_size=10,
    max_overflow=20,
    echo=False,  # Set to True for SQL logging
    future=True
)

# Test connection
with engine.connect() as conn:
    result = conn.execute("SELECT 1")
    print("âœ… Database connection successful")
```

---

## Migration Version Compatibility

**Current Version:** 001 (v0.1)  
**Next Expected:** 002 (Performance tuning indexes) â†’ T02.3.3  
**Future:** 003+ (Classification, tagging schema extensions)

### Version Timeline

| Version | Task | Purpose | Status |
|---------|------|---------|--------|
| 001 | T02.3.2 | Initial schema | âœ… Complete |
| 002 | T02.3.3 | Performance tuning | ðŸŸ¡ Planned |
| 003+ | T02.4+, T02.5+ | Schema extensions | ðŸ”µ Future |

---

## Error Handling & Resilience

### Migration Failure Scenarios

| Scenario | Cause | Recovery | RTO |
|----------|-------|----------|-----|
| Connection fail | DB unavailable | Wait/retry | Manual |
| Permission denied | User lacks privileges | Grant permissions | < 1 min |
| Table exists | Migration ran twice | Idempotent (safe) | 0 min |
| Foreign key violation | Constraint error | Check data state | < 5 min |
| Disk full | Storage exhausted | Add storage | < 1 hour |

### Monitoring & Alerts

```sql
-- Monitor migration status
SELECT version_num, installed_on FROM alembic_version;

-- Monitor table sizes
SELECT schemaname, tablename, pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename))
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- Monitor index usage
SELECT indexname, idx_scan FROM pg_stat_user_indexes
WHERE schemaname = 'public'
ORDER BY idx_scan DESC;
```

---

## Downstream Task Dependencies

**T02.3.2 Blocks:**
- âœ… T02.3.3 (Performance Tuning) â€“ Requires V001 schema to exist
- âœ… T02.4.2+ (Classification) â€“ Audit log needed for tracking
- âœ… T02.5.1+ (Tagging) â€“ Schema needed for metadata storage

**Cannot Start T02.3.3 Until:** âœ… T02.3.2 complete (schema must exist)  
**Cannot Start T02.4.2 Until:** âœ… T02.3.1 + T02.3.2 complete  
**Cannot Start T02.5.1 Until:** âœ… T02.3.3 complete (indexes tuned)

---

## Operational Handoff Notes (for QC-101 & DEV-033/DEV-024)

### For DEV-033 (T02.3.3 Performance Tuning):
- Schema is in place: 4 tables, 8 indices
- Performance baseline captured (creation time: 1.042s)
- Ready for: query analysis, index tuning, query plan optimization

### For DEV-024 / DEV-027 (T02.2.3 Dedup Implementation):
- hash_index table ready for O(1) duplicate lookup
- Foreign keys + cascade deletes verified
- Ready for: batch import + hash computation + dedup logic

### For AGENT-002 / DATA-029 (T02.4.2-T02.4.3 Classification):
- audit_log table ready for action tracking
- documents.metadata_extracted (JSON) ready for classification results
- Ready for: prompt design + evaluation against populated schema

---

## DEV-034 Handoff Checklist âœ…

- [x] Schema created per T02.3.1 design
- [x] Migrations tested (forward, rollback, idempotency)
- [x] Foreign keys verified (referential integrity enforced)
- [x] Cascade deletes tested
- [x] All 8 indices created
- [x] Connection pooling config defined
- [x] Error handling scenarios documented
- [x] RTO/RPO targets met (< 2 min rollback, 0 data loss)
- [x] Operational runbooks written
- [x] Integration points mapped

---

**Status:** âœ… **COMPLETE â€“ READY FOR T02.3.3 & DOWNSTREAM TASKS**  
**Completion Time:** 2026-01-15T10:45Z
