# Architecture Design: Document Import Engine v1

**Task:** T02.1.2_JD-DEV024_DesignImportEngine  
**Owner:** DEV-024 (Deliverables Manager)  
**Date:** 2026-01-14  
**Status:** ✅ DESIGN COMPLETE

---

## Executive Summary

Import engine processes PDF and DOCX documents in batches (up to 100 docs) with sub-500ms per-document latency. Uses pdfplumber (PDF) + python-docx (DOCX) for parsing, SQLite for metadata storage, and local-only processing (zero network calls).

---

## High-Level Architecture

```
User Input (Folder/Batch)
    ↓
[Batch Validator]
    ├─ Check file types (PDF, DOCX only)
    ├─ Enforce batch size limit (≤100)
    ├─ Calculate total size
    └─ Early reject if invalid
    ↓
[Import Orchestrator] (async queue)
    ├─ Sequential or parallel document processing?
    ├─ Error handling strategy (continue on partial failure)
    └─ Audit trail logging (what succeeded/failed, why)
    ↓
[Document Parser Module]
    ├─ PDF Parser (pdfplumber)
    │   ├─ Extract text per page
    │   ├─ Capture metadata (page count, title if available)
    │   └─ Handle corrupted PDFs gracefully
    ├─ DOCX Parser (python-docx)
    │   ├─ Extract text from paragraphs + tables
    │   ├─ Capture metadata (author, created date)
    │   └─ Handle corrupted DOCX gracefully
    └─ Returns: {text, metadata, page_count, parse_time_ms}
    ↓
[Hash Computation] (SHA-256, for dedup later)
    ├─ Hash full document content
    ├─ Store hash for dedup check (D02.2)
    └─ < 50ms per document
    ↓
[Metadata Storage] (SQLite)
    ├─ Write to documents table
    ├─ Write hash to hash_index table
    ├─ Write import event to audit_log
    └─ Transactional: all or nothing per document
    ↓
[Response]
    └─ Return {success, document_id, metadata, errors}
```

---

## Module Decomposition

### 1. **Batch Validator Module**
- **Input:** File list + batch metadata
- **Processing:**
  - Validate file extensions (.pdf, .docx only)
  - Count files (must be ≤100)
  - Sum file sizes (should warn if >1GB total)
  - Early rejection if constraints violated
- **Output:** Validated file list OR error with reason
- **Error Handling:** Return 400-level error with reason (unsupported type, batch too large)
- **Performance:** < 100ms for batch validation

### 2. **PDF Parser Module**
- **Input:** PDF file path
- **Processing:**
  - Open with pdfplumber
  - Iterate pages, extract text
  - Capture metadata (page count, PDF properties if available)
  - Handle corrupted PDFs (return partial + error flag)
- **Output:** `{text: str, page_count: int, metadata: dict, error: str|null, parse_time_ms: int}`
- **Timeout:** 2 seconds per PDF (kill if exceeds)
- **Performance Target:** < 500ms per PDF
- **Libraries:** `pdfplumber` (already chosen in T02.1.1)

### 3. **DOCX Parser Module**
- **Input:** DOCX file path
- **Processing:**
  - Open with python-docx
  - Extract text from paragraphs + all tables
  - Capture metadata (author, creation date, last modified)
  - Handle corrupted DOCX (return partial + error flag)
- **Output:** `{text: str, metadata: dict, error: str|null, parse_time_ms: int}`
- **Timeout:** 2 seconds per DOCX
- **Performance Target:** < 500ms per DOCX
- **Libraries:** `python-docx` (already chosen in T02.1.1)

### 4. **Hash Computation Module**
- **Input:** Full document text
- **Processing:**
  - Compute SHA-256 hash
  - Return hash string
- **Output:** `{hash: str, compute_time_ms: int}`
- **Performance Target:** < 50ms per document
- **Integration:** Used by D02.2 (Dedup) to check for duplicates
- **Note:** Hash is computed but NOT checked at this stage (dedup is separate task T02.2.1+)

### 5. **Metadata Storage Module**
- **Input:** Parsed document data + hash
- **Processing:**
  - Connect to SQLite database
  - Validate schema (documents, hash_index, audit_log tables exist)
  - Insert into documents table
  - Insert hash into hash_index (UNIQUE, so dedup checks happen later)
  - Insert audit log entry (who imported, when, result)
  - Commit transaction
- **Output:** `{document_id: int, success: bool, error: str|null, storage_time_ms: int}`
- **Error Handling:** Transactional (rollback on error, return specific error message)
- **Performance Target:** < 100ms per document (dominated by disk I/O)

### 6. **Batch Orchestrator Module**
- **Input:** Validated batch + configuration (sequential vs parallel, error strategy)
- **Processing:**
  - For each document in batch:
    - Call appropriate parser (PDF or DOCX)
    - Compute hash
    - Store metadata
    - Log result (success/error)
  - Aggregate results
  - Return summary (total: X, success: Y, failed: Z, total_time: T)
- **Output:** Batch summary + per-document status
- **Error Strategy:** Partial success model
  - If 1 doc fails, continue processing rest
  - Return overall status + per-document errors
  - Batch is NOT all-or-nothing (allow partial success)
- **Performance Target:** Total batch < 60 seconds (100 docs × 500ms + orchestration overhead)

---

## Database Schema Integration

**T02.1.2 assumes these tables exist (designed in T02.3.1):**

```sql
CREATE TABLE documents (
  id INTEGER PRIMARY KEY,
  filename TEXT NOT NULL,
  import_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  source TEXT,  -- "batch_import", "manual", etc.
  document_type TEXT,  -- "pdf", "docx", etc.
  page_count INTEGER,
  sha256_hash TEXT NOT NULL,
  metadata_json TEXT,  -- JSON: {author, title, created_date, etc.}
  import_status TEXT,  -- "success", "partial", "error"
  error_message TEXT
);

CREATE TABLE hash_index (
  id INTEGER PRIMARY KEY,
  sha256_hash TEXT UNIQUE NOT NULL,
  first_document_id INTEGER REFERENCES documents(id),
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE audit_log (
  id INTEGER PRIMARY KEY,
  event_type TEXT,  -- "import_batch_start", "import_batch_complete", "import_document_success", etc.
  document_id INTEGER REFERENCES documents(id),
  details_json TEXT,  -- {batch_id, duration_ms, file_size, etc.}
  timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**T02.1.2 responsibilities:**
- Populate documents table
- Populate hash_index table (with document's hash)
- Write audit_log entries for each import event

---

## Deployment & Configuration

### Environment Setup
- Python 3.10+
- SQLite3 (included with Python)
- External libraries:
  - `pdfplumber` (PDF parsing)
  - `python-docx` (DOCX parsing)
  - `sqlalchemy` (ORM for database access, optional but recommended)

### Configuration (as code or env vars)
```python
IMPORT_CONFIG = {
    "max_batch_size": 100,
    "max_total_size_mb": 1000,
    "max_doc_size_mb": 50,
    "parse_timeout_seconds": 2,
    "supported_types": ["pdf", "docx"],
    "hash_algorithm": "sha256",
    "partial_success_allowed": True,
    "database_path": "./documents.db",
}
```

### Local-Only Compliance
- ✅ No network calls
- ✅ All processing in-process
- ✅ Database local SQLite (no cloud)
- ✅ No API calls to external services

---

## Error Handling Strategy

| Scenario | Action | Result |
|----------|--------|--------|
| Unsupported file type | Reject early in validator | Return error, skip file, continue batch |
| Batch > 100 docs | Reject in validator | Return error, stop batch |
| PDF corrupted | Try parse, return partial + error flag | Store partial text, mark in audit |
| DOCX corrupted | Try parse, return partial + error flag | Store partial text, mark in audit |
| Parser timeout (>2s) | Kill process, return error | Skip document, continue batch |
| Hash collision | Log warning, still import both (dedup is later task) | Audit log notes potential duplicate |
| DB insert fails | Rollback transaction, return error | Skip document, continue batch |

---

## Performance Targets & Benchmarks

| Operation | Target | Notes |
|-----------|--------|-------|
| Batch validation | <100ms | Validation only, no I/O |
| PDF parse (avg) | <500ms | Includes text extraction |
| DOCX parse (avg) | <500ms | Includes table extraction |
| Hash computation | <50ms | SHA-256 on full text |
| DB insert | <100ms | Transactional write |
| Full document cycle | <1200ms | Parse + hash + store |
| Batch of 100 | <60s | 100 × 1200ms + overhead |

**Measurement:** Use `time.perf_counter()` around each operation, log duration to audit_log.

---

## Integration Points

### D02.2 (Deduplication) – T02.2.2+
- T02.1.2 computes hash and stores in hash_index
- T02.2.x will query hash_index to detect duplicates
- **No blocking dependency:** T02.1.2 completes independent of D02.2

### D02.3 (Metadata Storage) – T02.3.x
- T02.1.2 writes to documents, hash_index, audit_log
- Schema must exist (created in T02.3.1)
- **Blocking dependency:** T02.3.1 must complete before T02.1.2 can execute

### D02.4 (Classification) – T02.4.x
- T02.1.2 populates documents with raw text
- T02.4.x will classify documents by type (Invoice, Contract, etc.)
- **No blocking dependency:** T02.1.2 completes independent

---

## Design Review Checklist

- [x] All modules defined with clear inputs/outputs
- [x] Error handling strategy specified for each module
- [x] Performance targets established
- [x] Database schema integration points identified
- [x] Local-only compliance confirmed (no network calls)
- [x] Partial success model allows batch processing robustness
- [x] Dependencies on D02.3 schema clear
- [x] Ready for T02.1.3 (implementation task)

---

**Document Status:** Design COMPLETE. Ready for T02.1.3 implementation.  
**Next:** T02.1.3_JD-DEV024_ImplementBatchImport (implementation task)
