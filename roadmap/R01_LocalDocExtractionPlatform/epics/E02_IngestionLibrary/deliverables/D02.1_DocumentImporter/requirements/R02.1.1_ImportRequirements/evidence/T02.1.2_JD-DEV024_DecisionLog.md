# Design Decision Log: Import Engine Architecture

**Task:** T02.1.2_JD-DEV024_DesignImportEngine  
**Owner:** DEV-024 (Deliverables Manager)  
**Date:** 2026-01-14

---

## All Design Decisions

### DECISION 1: Parser Approach (Sequential vs Parallel)

**Question:** Should the system parse documents sequentially (one at a time) or in parallel (multiple at once)?

**Options Considered:**
1. **Sequential** – Parse one document, then next
   - Pros: Simple error handling, lower memory use, predictable behavior
   - Cons: Slower total throughput (100 docs = 100 × 500ms = 50 seconds)

2. **Parallel (async/await)** – Parse multiple documents concurrently
   - Pros: Faster throughput (could halve time with 2 workers)
   - Cons: Complex error handling, harder debugging, thread contention on I/O

3. **Hybrid** – Parallel parsing, sequential storage
   - Pros: Balance speed + simplicity
   - Cons: Adds queuing complexity

**Decision:** ✅ **SEQUENTIAL**

**Rationale:**
- T02.1.1 acceptance criteria: "< 500ms per document" not "total batch < X seconds"
- Error handling simpler (debug which doc failed, exactly when)
- Memory predictable (no accumulating parsed docs in queue)
- Storage is sequential anyway (SQLite serializes writes)
- Parallel gains minimal if network/I/O is bottleneck (which it is)

**Impact:** Batch of 100 takes ~60 seconds (acceptable for import-once workflow)

---

### DECISION 2: Library Choices (pdfplumber, python-docx, SQLite)

**Question:** Which libraries should we use for PDF parsing, DOCX parsing, and database?

**Options for PDF:**
1. PyPDF2 – Pure Python, handles most PDFs
2. **pdfplumber** – Lighter, good text extraction, handles corrupted files
3. PyMuPDF (fitz) – Fast but requires compilation
4. pikepdf – Focused on PDF manipulation, not text

**Options for DOCX:**
1. **python-docx** – Standard, handles paragraphs + tables
2. python-pptx – For slides (not needed)
3. docx2pdf – For conversion (not needed)
4. Manual XML parsing – Too error-prone

**Options for Database:**
1. **SQLite** – Embedded, no setup, perfect for local-only
2. PostgreSQL – Overkill for single machine
3. MongoDB – Schema-less but overhead
4. File-based (JSON/CSV) – No query capability

**Decision:** ✅ **pdfplumber, python-docx, SQLite**

**Rationale:**
- pdfplumber: Handles edge cases, corrupted PDFs, good text extraction
- python-docx: Industry standard, well-documented, handles complex DOCX
- SQLite: Zero setup, local-only, queryable, perfect for E02 scope

**Impact:** Standard Python stack, easy to test, no vendor lock-in

---

### DECISION 3: Error Handling Strategy

**Question:** If a document fails to parse, should we fail the entire batch or continue with remaining docs?

**Options:**
1. **All-or-nothing** – If any doc fails, reject entire batch
   - Pros: Simple, no partial state
   - Cons: User frustrated by one bad file breaking entire import

2. **Partial success** – Skip failed docs, import successful ones
   - Pros: User-friendly, robust
   - Cons: Needs clear logging of what failed and why

3. **Quarantine** – Move failed docs to separate folder, re-import later
   - Pros: No data lost
   - Cons: Extra complexity, deferred problem

**Decision:** ✅ **PARTIAL SUCCESS**

**Rationale:**
- T02.1.1 acceptance criteria explicitly say "partial success model"
- Stakeholders want robustness (import as much as possible)
- Audit log will record what failed and why (debugging possible)
- User can re-submit failed docs later with more info

**Impact:** Error handling in Batch Orchestrator must aggregate per-doc results, not fail fast

---

### DECISION 4: Hash Computation Timing

**Question:** When should we compute the document hash – at import time or dedup time?

**Options:**
1. **At import time** – Compute hash immediately, store in hash_index
   - Pros: Dedup is fast (just lookup), dedup can run independent of import
   - Cons: Extra computation at import (but < 50ms target)

2. **At dedup time** – Only compute hash when checking for duplicates
   - Pros: Skip hashing for unique imports
   - Cons: Dedup is slower, must coordinate with import task

**Decision:** ✅ **AT IMPORT TIME**

**Rationale:**
- < 50ms hash computation is negligible
- Enables D02.2 to run independently without touching import code
- Dedup becomes simple lookup (excellent separation of concerns)
- No performance impact (<50ms is buried in 1200ms total cycle)

**Impact:** T02.1.2 stores hash; T02.2.2 uses pre-computed hashes

---

### DECISION 5: Batch Size Limit

**Question:** What should be the maximum batch size? (T02.1.1 said "up to 100 docs")

**Options:**
1. 50 docs – Conservative, faster
2. **100 docs** – T02.1.1 requirement, balances throughput
3. 200+ docs – Ambitious, may hit memory/timeout limits
4. Unlimited – Risk of hangs/memory exhaustion

**Decision:** ✅ **100 DOCUMENTS**

**Rationale:**
- Explicitly in T02.1.1 acceptance criteria
- 100 × 500ms per doc = 60 seconds total (reasonable for import-once workflow)
- Memory: 100 × 1MB avg doc = ~100MB (well within typical machines)
- If user needs more, they can split into multiple batches

**Impact:** Batch Validator enforces max 100; rejects batches >100 with clear error

---

### DECISION 6: Per-Document Timeout

**Question:** How long should we allow a single document to parse before killing it?

**Options:**
1. 1 second – Very aggressive, may kill slow valid docs
2. **2 seconds** – Reasonable for most PDFs/DOCX, catches hangs
3. 5 seconds – Lenient, allows large files
4. No timeout – Risk of infinite hangs

**Decision:** ✅ **2 SECONDS**

**Rationale:**
- Target is < 500ms per doc (2s is 4× buffer)
- Catches pathological cases (infinite loops in parser)
- Allows ~20 pages of dense PDFs before timeout
- Clear error message to user: "Document took too long to parse"

**Impact:** Use `signal.alarm()` or thread timeout to enforce

---

### DECISION 7: Storage Transaction Model

**Question:** Should we commit each document to DB independently or batch them?

**Options:**
1. **Per-document commits** – Commit each doc as it finishes
   - Pros: Partial recovery if process crashes mid-batch
   - Cons: More I/O operations

2. **Batch commits** – Commit all 100 docs at end
   - Pros: Fewer I/O operations
   - Cons: Lose all if crash mid-batch

**Decision:** ✅ **PER-DOCUMENT COMMITS**

**Rationale:**
- Aligns with partial success model
- If import crashes mid-batch, recovered docs aren't lost
- SQLite is fast enough for per-doc commits (< 100ms each)
- Audit trail shows exact timestamp of each import

**Impact:** Batch Orchestrator calls storage module per document, commits immediately

---

### DECISION 8: Database Schema Coupling

**Question:** Can T02.1.2 proceed before T02.3.1 (Schema Design) completes?

**Options:**
1. Yes, assume schema – Risk: schema changes break implementation
2. **No, wait for T02.3.1** – Safe, but blocks execution

**Decision:** ✅ **T02.1.2 BLOCKS ON T02.3.1 COMPLETION**

**Rationale:**
- Schema must exist before any writes
- Can't implement without knowing table structure
- T02.3.1 is on critical path (affects D02.5)
- Spec references schema from T02.3.1; reuse is best practice

**Impact:** T02.1.3 (implementation) cannot start until T02.3.1 completes

---

## Summary of All Decisions

| # | Decision | Choice | Rationale |
|---|----------|--------|-----------|
| 1 | Parsing strategy | Sequential | Simple, predictable error handling |
| 2 | Libraries | pdfplumber, python-docx, SQLite | Standard, proven, local-only |
| 3 | Error handling | Partial success | Per T02.1.1 requirement, user-friendly |
| 4 | Hash timing | At import time | Fast dedup downstream, clear separation |
| 5 | Batch limit | 100 documents | Per T02.1.1 requirement |
| 6 | Parse timeout | 2 seconds | 4× buffer above 500ms target |
| 7 | Storage model | Per-document commits | Aligns with partial success |
| 8 | Schema coupling | Block on T02.3.1 | Safe, schema must exist |

---

## No Unresolved Design Questions

All architectural decisions have been made and justified. Ready for implementation.

---

**Document Status:** Decision Log COMPLETE.  
**Next:** Implementation (T02.1.3) can proceed when T02.3.1 schema design is complete.
